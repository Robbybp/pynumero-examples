{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3ede394-098a-4130-8ce3-d907348bf0ba",
   "metadata": {},
   "source": [
    "# Example 1: Constructing the reduced Hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c09a255-759d-46ae-94bc-846ec8ad72d0",
   "metadata": {},
   "source": [
    "In this example, we will solve a model and check the eigenvalues of the reduced Hessian at the solution. The purpose of this example is to (a) practice using `PyomoNLP` and (b) introduce you to the reduced Hessian, which can be useful for debugging regularization coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4289b01-ea4e-4508-9dd5-e450bc7f8937",
   "metadata": {},
   "source": [
    "We start with some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b3e2ef-3fcf-4b64-95d9-39aeca8bd1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyomo.environ as pyo\n",
    "import pyomo.environ as pyo\n",
    "from pyomo.common.collections import ComponentSet\n",
    "from pyomo.contrib.incidence_analysis import IncidenceGraphInterface\n",
    "from pyomo.contrib.pynumero.interfaces.pyomo_nlp import PyomoNLP\n",
    "from svi.auto_thermal_reformer.fullspace_flowsheet import make_optimization_model\n",
    "import numpy as np\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ecff96-28ff-4967-b466-4e6f493d8b43",
   "metadata": {},
   "source": [
    "The model we will solve comes from the \"surrogate-vs-implicit\" repository, available [here](https://github.com/robbybp/surrogate-vs-implicit). We have imported this package above, and will construct an autothermal reforming flowsheet model using its function `make_optimization_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbb185c-f703-44ed-8b48-ca86832c3067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_and_solve_model(**kwds):\n",
    "    # First, we make our model\n",
    "    P = kwds.pop(\"P\", 1.5e6)\n",
    "    # With X = 0.95, we converge. With X = 0.94, we don't. Go figure\n",
    "    X = kwds.pop(\"X\", 0.94)\n",
    "    m = make_optimization_model(X, P)\n",
    "    m.ipopt_zL_out = pyo.Suffix(direction=pyo.Suffix.IMPORT)\n",
    "    m.ipopt_zU_out = pyo.Suffix(direction=pyo.Suffix.IMPORT)\n",
    "    m.dual = pyo.Suffix(direction=pyo.Suffix.IMPORT_EXPORT)\n",
    "    # Does the model solve?\n",
    "    solver = pyo.SolverFactory(\"cyipopt\", options=kwds)\n",
    "    res = solver.solve(m, tee=True)\n",
    "    # Converges infeasible in 900 iter\n",
    "    return m, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ae6ed2-0e6b-450c-9069-504fac12c87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, res = make_and_solve_model(X=0.95)\n",
    "pyo.assert_optimal_termination(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33773846-72e7-4491-a39e-96c7a9db8c8f",
   "metadata": {},
   "source": [
    "Now we'll construct a `PyomoNLP` at the solution. This will give us access to the derivative matrices we'll need to construct the reduced Hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400f669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = PyomoNLP(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069bf622",
   "metadata": {},
   "source": [
    "The reduced Hessian relies on knowledge of our \"degrees of freedom.\" We'll assume we know what these are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cbefdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dof_varnames = [\n",
    "    \"fs.reformer_bypass.split_fraction[0.0,bypass_outlet]\",\n",
    "    \"fs.reformer_mix.steam_inlet_state[0.0].flow_mol\",\n",
    "    \"fs.feed.properties[0.0].flow_mol\",\n",
    "]\n",
    "dofvars = [m.find_component(name) for name in dof_varnames]\n",
    "dofvar_set = ComponentSet(dofvars)\n",
    "basicvars = [v for v in nlp.get_pyomo_variables() if v not in dofvar_set]\n",
    "print(f\"N. dof variables:   {len(dofvars)}\")\n",
    "print(f\"N. basic variables: {len(basicvars)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205ea1ec",
   "metadata": {},
   "source": [
    "We need the Jacobian of our equality constraints with respect to \"basic variables\" to be nonsingular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de74f9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eqcons = nlp.get_pyomo_equality_constraints()\n",
    "print(f\"N. equality constraints: {len(eqcons)}\")\n",
    "basic_submatrix = nlp.extract_submatrix_jacobian(basicvars, eqcons)\n",
    "try:\n",
    "    lu = sp.sparse.linalg.splu(basic_submatrix.tocsc())\n",
    "    print(f\"Eq. Jac. has rank at least {len(basicvars)}\")\n",
    "except RuntimeError:\n",
    "    # This matrix being singular doesn't prove anything about the rank.\n",
    "    # We could have just chosen a bad set of degrees of freedom.\n",
    "    print(\"Eq. Jac. is not (necessarily) full row rank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b910dc",
   "metadata": {},
   "source": [
    "This proves that our equality Jacobian is full row rank. Now we can start worrying about the reduced Hessian.\n",
    "When constructing anything involving the Hessian of the Lagrangian, it's always a good idea to construct the\n",
    "gradient of the Lagrangian and make sure we have the right sign convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8198f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_of_lagrangian(\n",
    "    nlp,\n",
    "    primal_lb_multipliers,\n",
    "    primal_ub_multipliers,\n",
    "):\n",
    "    grad_obj = nlp.evaluate_grad_objective()\n",
    "    jac = nlp.evaluate_jacobian()\n",
    "    duals = nlp.get_duals()\n",
    "    conjac_term = jac.transpose().dot(duals)\n",
    "    grad_lag = (\n",
    "        - grad_obj\n",
    "        - conjac_term\n",
    "        + primal_lb_multipliers\n",
    "        - primal_ub_multipliers\n",
    "    )\n",
    "    return grad_lag\n",
    "\n",
    "lbmult = [m.ipopt_zL_out[x] for x in nlp.get_pyomo_variables()]\n",
    "ubmult = [m.ipopt_zU_out[x] for x in nlp.get_pyomo_variables()]\n",
    "gradlag = get_gradient_of_lagrangian(nlp, lbmult, ubmult)\n",
    "print(max(abs(gradlag)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5856db58",
   "metadata": {},
   "source": [
    "Looks good. Now we can start working on the reduced Hessian. The reduced Hessian is the Hessian of the Lagrangian projected onto the null space of the equality constraint Jacobian. We'll start by constructing the Hessian of the Lagrangian, with variables in the order `(dofvars, basicvars)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6916d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "varorder = dofvars + basicvars\n",
    "varindices = nlp.get_primal_indices(varorder)\n",
    "hess = nlp.extract_submatrix_hessian_lag(varorder, varorder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9776dc2e",
   "metadata": {},
   "source": [
    "This is the Hessian of the Lagrangian, but we're still missing terms for the bounds and inequalities, i.e., $\\Sigma_x = X^{-1}V$, where $X$ is the difference between $X$ and its bound and $V$ is the diagonal matrix of bound multipliers. The term for inequalities is $\\nabla_x g^T G^{-1} \\Lambda_g \\nabla_x g$, which we get by pivoting the KKT matrix on $G$, the diagonal matrix of inequality constraint values. ($\\Lambda_g$ is the diagonal matrix of inequality constraint multipliers.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079803d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we'll get the bound terms\n",
    "lbmult = sp.sparse.diags([m.ipopt_zL_out[v] for v in nlp.get_pyomo_variables()])\n",
    "ubmult = sp.sparse.diags([m.ipopt_zU_out[v] for v in nlp.get_pyomo_variables()])\n",
    "primal_val = nlp.get_primals()[varindices]\n",
    "primal_lb = nlp.primals_lb()[varindices]\n",
    "primal_ub = nlp.primals_ub()[varindices]\n",
    "active_lbs = np.where(primal_lb == primal_val)[0]\n",
    "active_ubs = np.where(primal_ub == primal_val)[0]\n",
    "# Push values into interior if bounds are exactly active, so we don't divide by zero\n",
    "primal_val[active_lbs] += 1e-8\n",
    "primal_val[active_ubs] -= 1e-8\n",
    "primal_ubslack_inv = sp.sparse.diags(1 / (primal_ub - primal_val))\n",
    "primal_lbslack_inv = sp.sparse.diags(1 / (primal_val - primal_lb))\n",
    "bound_term = lbmult @ primal_ubslack_inv + ubmult @ primal_lbslack_inv\n",
    "\n",
    "# Now we construct the inequality term\n",
    "ineqcons = nlp.get_pyomo_inequality_constraints()\n",
    "ineq_jac = nlp.extract_submatrix_jacobian(varorder, ineqcons)\n",
    "ineq_val = nlp.evaluate_ineq_constraints()\n",
    "ineq_val_inv = sp.sparse.diags(1 / ineq_val)\n",
    "ineq_duals = sp.sparse.diags(nlp.get_duals_ineq())\n",
    "ineq_term = ineq_jac.transpose() @ ineq_val_inv @ ineq_duals @ ineq_jac\n",
    "\n",
    "# Now we add these to our Hessian\n",
    "hess += bound_term\n",
    "# We subtract the inequality term as our inequalities are `g(x) <= 0` (whereas we have\n",
    "# written the bound constraints as \">=\" constraints, e.g., `x - x^L >= 0`).\n",
    "hess -= ineq_term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6084cd6f",
   "metadata": {},
   "source": [
    "Now we can construct the reduced Hessian. Recall that the reduced Hessian is the Hessian of the Lagrangian projected onto the null space of the equality constraint Jacobian, i.e., $Z^T \\mathcal{H} Z$, where\n",
    "$Z$ is a basis for the equality Jacobian's null space. Null space bases are not unique, but we can get\n",
    "*a* null space basis from the partition of our variables into \"degrees of freedom\" and \"basic.\" See [here](https://epubs.siam.org/doi/10.1137/0607059) for more information on the problem of computing a null space basis\n",
    "of a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca35ce21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dof_submatrix = nlp.extract_submatrix_jacobian(dofvars, eqcons)\n",
    "ndof = len(dofvars)\n",
    "dof_nullbasis = np.identity(ndof)\n",
    "lu = sp.sparse.linalg.splu(basic_submatrix.tocsc())\n",
    "basic_nullbasis = - lu.solve(dof_submatrix.toarray())\n",
    "nullbasis = np.vstack((dof_nullbasis, basic_nullbasis))\n",
    "rh = nullbasis.transpose() @ hess @ nullbasis\n",
    "print(\"Reduced Hessian:\")\n",
    "print(rh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfea5f33",
   "metadata": {},
   "source": [
    "We have the reduced Hessian. Now let's check its eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a94e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "evals, evecs = np.linalg.eig(rh)\n",
    "print(evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b9663c",
   "metadata": {},
   "source": [
    "We have one (slightly) negative eigenvalue. Is that a problem? Your mileage may vary. Inertia, especially of small eigenvalues, can be fairly sensitive to the method used to compute it.\n",
    "\n",
    "The reduced Hessian is an important object in the theory of nonlinear optimization, mostly for ensuring that\n",
    "we are taking a descent direction.\n",
    "The magnitude of the eigenvalues here tells us that this problem is poorly scaled (with respect to these three degrees of freedom &mdash; maybe another choice is better scaled).\n",
    "Constructing this matrix can be useful for debugging large regularization coefficients that you might see when\n",
    "solving with Ipopt.\n",
    "If you can confirm that these regularization coefficients are caused by a large, negative eigenvalue in the reduced\n",
    "Hessian, you can potentially correct it by adding curvature in this eigenvalue's coordinate (say, by adding a bound on the corresponding variable).\n",
    "\n",
    "That's the end of this tutorial! I encourage you to copy the code in the `example1.py` file and try this out on your own nonlinear optimization problems. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
